<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Data Preprocess</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo">China-US <strong>Trade War</strong></a>
									<ul class="icons">
										<li><a href="#" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
										<li><a href="#" class="icon brands fa-snapchat-ghost"><span class="label">Snapchat</span></a></li>
										<li><a href="#" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
										<li><a href="#" class="icon brands fa-medium-m"><span class="label">Medium</span></a></li>
									</ul>
								</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1>Data Preprocess</h1>
									</header>

									<p><span class="image left"><img src="images/news_clean.png" alt="Data Cleaning Illustration" /></span>
										<h2>Data Cleaning</h2>
										<p>We performed data cleaning on the dataset of global media reports on the China–US trade war.</p>
										<p>First, we manually corrected apparent anomalies in the <code>PTime</code> field and used <code>DTime</code> to fill in missing <code>PTime</code> values.
										Then, we removed records with clearly abnormal formats or content.
										Finally, for duplicate news entries with identical sources and content, only one copy was retained to eliminate redundancy.
										</p>
									</p>
									<hr class="major" />
									<h2>Source Country Identification</h2>
									<h3>Method Description</h3>
									<p>We use the URL field of news articles and the large language model API provided by DeepSeek to perform intelligent analysis and structured extraction of news sources.</p>
									<p><span class="image right"><img src="images/news_nation.png" alt="" /></span>
										<p>For each news article, we construct a prompt and request the model to determine the country of origin of the news source, extract the full English name of the country, its ISO 3166-1 three-letter code, and the latitude and longitude information of its capital. The model responds with structured data in JSON format. We use regular expressions to clean the data and deserialize it to extract key fields, which are then written to the corresponding rows in a CSV file.</p>
										<p>This method offers strong flexibility and generalization capabilities, making it suitable for complex source scenarios where domain name hard matching is not feasible. The entire process is controlled by a Python script, which includes exception handling and request rate-limiting logic to ensure stable and reliable batch calls.</p>
										<!-- <h3>Sample Code</h3> -->
										<!-- <p>https://github.com/RubyZh/TradeWarVisualization/tree/main/preprocessing/nation.py</p> -->
										<ul class="actions">
											<li><a href="https://github.com/RubyZh/TradeWarVisualization/tree/main/preprocessing/nation.py" class="button">Sample Code</a></li>
										</ul>
									</p>
									<hr class="major" />
									<h2>Automatic News Summarization</h2>
									<h3>Model Introduction</h3>
									<p>The <code>facebook/bart-large-cnn model</code> is a large-scale text summarization model developed by Facebook, based on the BART architecture. </p>
									<p><span class="image left"><img src="images/news_summary.png" alt="Data Cleaning Illustration" /></span>
										<p>It has been fine-tuned on the CNN/Daily Mail news summarization dataset and is well-suited for tasks such as news summarization, document summarization, and other natural language generation applications. BART combines a bidirectional encoder with an autoregressive decoder, and adopts a self-supervised pretraining strategy in which corrupted input sequences are reconstructed.</p>
										<p>After fine-tuning, the model demonstrates strong performance in generating coherent, informative, and readable news summaries. Compared to similar models such as T5 and RoBERTa, it exhibits certain advantages in contextual understanding and summarization quality.</p>
										<p><strong>Note</strong>: The default maximum input length of the model is 1024 tokens. For decoding, it is recommended to set do_sample=False and optionally use beam search to ensure stable and high-quality summaries.</p>
										<h3>Model Application</h3>
										<p>The model is loaded using Hugging Face's pipeline interface.</p>
										<p>After tokenizing the input text, if the token count is fewer than 100, the original text is retained; if it exceeds the predefined limit of <code>MAX_TOKENS=1000</code>, it is truncated to this length to avoid poor summary quality due to overly short input or decreased inference efficiency due to overly long input.</p>
										<p>During inference, the summary length is constrained to be no less than 100 tokens and no more than 250 tokens, ensuring the output remains within a reasonable range and is deterministic. In case of any exception, an error flag is returned. All generated summaries are written back to the <code>Summary</code> field of the dataset and exported as a CSV file.</p>
										
										<!-- <h3>Model Resource Link</h3> -->
										<!-- <p>https://huggingface.co/facebook/bart-large-cnn</p> -->
										<ul class="actions">
											<li><a href="https://huggingface.co/facebook/bart-large-cnn" class="button primary">Model Resource</a></li>
										</ul>
									</p>

									<hr class="major" />
									<h2>Keyword extraction</h2>
									<h3>Model Introduction</h3>
									<p>YAKE is a lightweight, unsupervised automatic keyword extraction method that uses statistical features of the text to identify the most important keywords from a document. It is applicable to multiple languages and domains, and is not limited by the size of the text. It is currently open-source on GitHub.</p>
									<p><span class="image right"><img src="images/news_keyword.png" alt="Data Cleaning Illustration" /></span>
										<h3>Model Application</h3>
										<p>First, we installed the YAKE library using <code>pip install yake</code>. During usage, we set the parameters as follows: the language was set to English (<code>lan="en"</code>), the maximum keyword length was limited to three-word phrases (<code>n=3</code>), the top 15 keywords were extracted (<code>top=15</code>), and the default deduplication method (such as <code>seqm</code>) was used to avoid redundant keywords. For each news article, the extracted results included keywords along with their importance scores (the lower the score, the more relevant the keyword). Additionally, we used YAKE's <code>TextHighlighter</code> tool to mark and highlight the keywords within the original text, making it easier for subsequent presentation and analysis.</p>
										
										<!-- <h3>Model Resource Link</h3> -->
										<!-- <p>https://github.com/LIAAD/yake</p> -->
										<ul class="actions">
											<li><a href="https://github.com/LIAAD/yake" class="button">Model Resource</a></li>
										</ul>
									</p>
									<hr class="major" />
									
									<h2>Emotion Recognition</h2>
									<h3>Model Introduction</h3>
									<p><span class="image right"><img src="images/news_emo.png" alt="Data Cleaning Illustration" /></span>
										<p><code>j-hartmann/emotion-english-distilroberta-base</code> is a lightweight English sentiment classification model trained on the DistilRoBERTa architecture. The model was fine-tuned on the GoEmotions dataset, provided by Google, which includes over 58,000 Reddit comments annotated with 28 fine-grained sentiment labels, later consolidated into 7 main categories (anger, disgust, fear, joy, neutral, sadness, surprise), making it suitable for various real-world sentiment understanding tasks.</p>
										<p>DistilRoBERTa is a distilled version of RoBERTa, which is smaller in size and faster in inference than the original model, while still retaining good semantic understanding capabilities. It is particularly suitable for handling large-scale text classification tasks in production environments. In actual evaluation, the model maintains high accuracy while having high computational efficiency, which can significantly improve the overall response speed and scalability of the emotion recognition process.</p>
										<p>When invoking this model via the Hugging Face platform, users can directly use the <code>pipeline("text-classification")</code> interface and enable the <code>return_all_scores=True</code> parameter to return scores for all labels, supporting advanced analysis needs such as multi-label ranking and confidence comparison, greatly enhancing the model's flexibility and interpretability in practical applications.</p>
										<h3>Model Application</h3>
										<p>Load the model through Hugging Face's pipeline interface and perform batch sentiment analysis on the news summary field. For each summary, the model outputs the corresponding multi-category sentiment scores. By sorting the scores, extract the top two sentiment labels and their confidence scores, and write them to the result file as Top1 and Top2 sentiment features, saved in UTF-8 encoding format as CSV.</p>
										
										<!-- <h3>Model Resource Link</h3> -->
										<!-- <p>https://huggingface.co/j-hartmann/emotion-english-distilroberta-base</p> -->
										<ul class="actions">
											<li><a href="https://huggingface.co/j-hartmann/emotion-english-distilroberta-base" class="button primary">Model Resource</a></li>
										</ul>
									</p>
									<hr class="major" />
									<h2>Our Dataset</h2>
									<table>
										<thead>
											<tr>
												<th>Field name</th>
												<th>Meaning</th>
											</tr>	
										</thead>
										<tbody>
											<tr>
												<td>UniqueID</td>
												<td>News unique identifier</td>
											</tr>
											<tr>
												<td>Title</td>
												<td>News Headline</td>
											</tr>
											<tr>
												<td>Author</td>
												<td>News author</td>
											</tr>
											<tr>
												<td>PTime</td>
												<td>Publish time</td>
											</tr>
											<tr>
												<td>DTime</td>
												<td>Grasping time</td>
											</tr>
											<tr>
												<td>Source</td>
												<td>News source platform</td>
											</tr>
											<tr>
												<td>URL</td>
												<td>News web page link</td>
											</tr>
											<tr>
												<td>Content</td>
												<td>The main content of the news</td>
											</tr>
											<tr>
												<td>Rank</td>
												<td>Source website influence rating</td>
											</tr>
											<tr>
												<td>CountryName</td>
												<td>Name of the country where the news source comes from (English)</td>
											</tr>
											<tr>
												<td>CountryISO_Code</td>
												<td>The ISO code of the news source country</td>
											</tr>
											<tr>
												<td>CapitalLatitude</td>
												<td>The latitude of the capital of the source country</td>
											</tr>
											<tr>
												<td>CapitalLongitude</td>
												<td>The longitude of the capital of the source country</td>
											</tr>
											<tr>
												<td>CCode</td>
												<td>COW country code</td>
											</tr>
											<tr>
												<td>USAgree</td>
												<td>The average of the ideal point consistent with the position of the United States</td>
											</tr>
											<tr>
												<td>ChinaAgree</td>
												<td>The average value of the ideal point consistent with China's position</td>
											</tr>
											<tr>
												<td>Summary</td>
												<td>The generated news summary</td>
											</tr>
											<tr>
												<td>Top1Emotion</td>
												<td>Main emotional tags</td>
											</tr>
											<tr>
												<td>Top1Score</td>
												<td>Main emotional confidence level</td>
											</tr>
											<tr>
												<td>Top2Emotion</td>
												<td>Secondary emotional labels</td>
											</tr>
											<tr>
												<td>Top2Score</td>
												<td>Secondary emotional confidence</td>
											</tr>
											<tr>
												<td>Key1-Key15</td>
												<td>Key words 1-15</td>
											</tr>
										</tbody>
									</table>
									<p>Our final dataset integrates multi-source information around the theme of the “US-China trade war,” covering rich dimensions such as news content, source country, summary, sentiment, and keywords. With this structured data, we can conduct comprehensive analyses from multiple angles, including sentiment distribution by country, sentiment and stance analysis of summary content, correlation analysis between media platform influence and reporting attitudes, and dissemination characteristics of China-US related topics across media in different countries. This dataset helps readers gain a clearer understanding of the development trajectory of the China-US trade war, the reporting perspectives of media in various countries, and the evolution of international public opinion through structured news analysis.</p>
									
									
								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">Homepage</a></li>
										<li><a href="DataPreprocess.html">Data preprocess</a></li>
										<li><a href="Timeline.html">Timeline</a></li>
										<li>
											<span class="opener">Media Report</span>
											<ul>
												<li><a href="Topics.html">Topics</a></li>
												<li><a href="Emotions.html">Emotions</a></li>
											</ul>
										</li>
										<li>
											<span class="opener">3-D Visualization</span>
											<ul>
												<li><a href="#">Lorem Dolor</a></li>
												<li><a href="#">Ipsum Adipiscing</a></li>
												<li><a href="#">Tempus Magna</a></li>
												<li><a href="#">Feugiat Veroeros</a></li>
											</ul>
										</li>
										<li><a href="#">Summary</a></li>
									</ul>
								</nav>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Ante interdum</h2>
									</header>
									<div class="mini-posts">
										<article>
											<a href="#" class="image"><img src="images/pic07.jpg" alt="" /></a>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
										</article>
										<article>
											<a href="#" class="image"><img src="images/pic08.jpg" alt="" /></a>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
										</article>
										<article>
											<a href="#" class="image"><img src="images/pic09.jpg" alt="" /></a>
											<p>Aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore aliquam.</p>
										</article>
									</div>
									<ul class="actions">
										<li><a href="#" class="button">More</a></li>
									</ul>
								</section>

							<!-- Section -->
								<section>
									<header class="major">
										<h2>Get in touch</h2>
									</header>
									<p>Sed varius enim lorem ullamcorper dolore aliquam aenean ornare velit lacus, ac varius enim lorem ullamcorper dolore. Proin sed aliquam facilisis ante interdum. Sed nulla amet lorem feugiat tempus aliquam.</p>
									<ul class="contact">
										<li class="icon solid fa-envelope"><a href="#">information@untitled.tld</a></li>
										<li class="icon solid fa-phone">(000) 000-0000</li>
										<li class="icon solid fa-home">1234 Somewhere Road #8254<br />
										Nashville, TN 00000-0000</li>
									</ul>
								</section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Untitled. All rights reserved. Demo Images: <a href="https://unsplash.com">Unsplash</a>. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>